{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaWPfLAZWOBiyTlWVryqfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krixna-Kant/Infosys-Springboard-AI-ML-Internship/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v9qVhNORbOS-"
      },
      "outputs": [],
      "source": [
        "# Importing PyTorch for building and training machine learning models\n",
        "import torch\n",
        "\n",
        "# Importing nn module to define neural network layers and architectures\n",
        "import torch.nn as nn\n",
        "\n",
        "# Importing optim module to adjust weights during training\n",
        "import torch.optim as optim\n",
        "\n",
        "# Importing torchvision for working with datasets and applying transformations\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Importing DataLoader to handle batching and shuffling of datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Importing matplotlib for plotting and visualizing data\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors (from [0, 255] to [0, 1])\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to the range [-1, 1]\n",
        "])\n",
        "\n",
        "# Load the MNIST training dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',  # Directory to store the data\n",
        "    train=True,     # Load the training set\n",
        "    download=True,  # Download the dataset if not already downloaded\n",
        "    transform=transform  # Apply the transformations defined above\n",
        ")\n",
        "\n",
        "# Load the MNIST test dataset\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',  # Directory to store the data\n",
        "    train=False,    # Load the test set\n",
        "    download=True,  # Download the dataset if not already downloaded\n",
        "    transform=transform  # Apply the transformations defined above\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO8rDBdJbi7R",
        "outputId": "479d1197-3bd9-44d4-aca5-c999c336795b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.81MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 153kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.45MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.62MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for the training dataset\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,   # Dataset to load\n",
        "    batch_size=64,   # Number of images per batch\n",
        "    shuffle=True     # Shuffle the data for better training performance\n",
        ")\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,   # Dataset to load\n",
        "    batch_size=64,  # Number of images per batch\n",
        "    shuffle=False   # No need to shuffle test data for evaluation\n",
        ")\n"
      ],
      "metadata": {
        "id": "eFADJBXmbts4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LeNet-5 model architecture as a subclass of nn.Module\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        # Define layers of LeNet-5 architecture:\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)  # First convolutional layer (input: grayscale)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)                  # Average pooling layer\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)                       # Second convolutional layer\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)                              # First fully connected layer (input size adjusted)\n",
        "        self.fc2 = nn.Linear(120, 84)                                      # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(84, 10)                                       # Output layer (10 classes)\n",
        "\n",
        "        self.relu = nn.ReLU()                                              # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))                                      # First convolution + ReLU activation\n",
        "        x = self.pool(x)                                                  # Pooling after first conv\n",
        "\n",
        "        x = self.relu(self.conv2(x))                                      # Second convolution + ReLU activation\n",
        "        x = self.pool(x)                                                  # Pooling after second conv\n",
        "\n",
        "        x = x.view(-1, 16 * 5 * 5)                                        # Flatten output for fully connected layers\n",
        "\n",
        "        x = self.relu(self.fc1(x))                                        # First fully connected layer + ReLU activation\n",
        "        x = self.relu(self.fc2(x))                                        # Second fully connected layer + ReLU activation\n",
        "\n",
        "        x = self.fc3(x)                                                   # Final output layer\n",
        "\n",
        "        return x                                                          # Return raw scores (no softmax yet)\n"
      ],
      "metadata": {
        "id": "_e6y-GTmbxxU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer based on device availability (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()                                     # Loss function combining softmax and negative log likelihood loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)                 # Adam optimizer for weight adjustment during training\n"
      ],
      "metadata": {
        "id": "GJ14gtN7b3eZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model over multiple epochs\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)     # Move data to device\n",
        "\n",
        "            outputs = model(images)                                     # Forward pass through model\n",
        "\n",
        "            loss = criterion(outputs, labels)                          # Calculate loss\n",
        "\n",
        "            optimizer.zero_grad()                                       # Zero gradients from previous iteration\n",
        "\n",
        "            loss.backward()                                             # Backward pass to calculate gradients\n",
        "\n",
        "            optimizer.step()                                            # Update weights based on gradients\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "5yET1XE2b6RF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model performance on test data\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0                                                        # Counter for correct predictions\n",
        "    total = 0                                                          # Counter for total predictions\n",
        "\n",
        "    with torch.no_grad():                                              # Disable gradient calculation during evaluation\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)     # Move data to device\n",
        "\n",
        "            outputs = model(images)                                     # Get predictions from model\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)                 # Get predicted class (index with highest score)\n",
        "\n",
        "            total += labels.size(0)                                    # Update total count\n",
        "            correct += (predicted == labels).sum().item()             # Update count of correct predictions\n",
        "\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")              # Print accuracy percentage\n"
      ],
      "metadata": {
        "id": "TwDqSOIhcApx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and evaluate its performance on test set\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y0VLeNHcELl",
        "outputId": "161ef4ff-c9c5-44d9-89ce-32a1423f1af2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3112\n",
            "Epoch 2/10, Loss: 0.0868\n",
            "Epoch 3/10, Loss: 0.0617\n",
            "Epoch 4/10, Loss: 0.0483\n",
            "Epoch 5/10, Loss: 0.0409\n",
            "Epoch 6/10, Loss: 0.0351\n",
            "Epoch 7/10, Loss: 0.0308\n",
            "Epoch 8/10, Loss: 0.0266\n",
            "Epoch 9/10, Loss: 0.0239\n",
            "Epoch 10/10, Loss: 0.0219\n",
            "Test Accuracy: 99.02%\n"
          ]
        }
      ]
    }
  ]
}